<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>交互式DPO算法浏览器</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;500;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Warm Neutral Harmony -->
    <!-- Application Structure Plan: The SPA is designed as a single-page, scrollable journey with a sticky navigation bar for easy access to different knowledge modules. The structure is thematic rather Pre-trainingthan linear: 1. A visual comparison of RLHF vs. DPO to immediately establish context and DPO's value proposition. 2. A breakdown of the core mathematical theory using interactive elements (hover-to-explain formulas) to make it digestible. 3. A simplified flowchart of the algorithm to bridge theory and code. 4. A clean presentation of the code itself. 5. A fully interactive simulator. 6. Two new LLM-powered features: a concept explainer and an application scenario generator, enhancing interactive learning and practical relevance. This structure allows users with different goals (e.g., theorist, practitioner) to navigate directly to their area of interest, while the simulator and LLM tools act as powerful, central tools for intuitive learning, which is far more effective than reading static text. -->
    <!-- Visualization & Content Choices: 
        - Report Info: RLHF vs DPO process. -> Goal: Compare. -> Viz: HTML/CSS diagram. -> Interaction: None. -> Justification: A clear, static visual is the fastest way to convey the structural difference and DPO's simplification.
        - Report Info: Mathematical formulas (Bradley-Terry, Reward, DPO Loss). -> Goal: Inform/Teach. -> Viz: Styled HTML text. -> Interaction: Hover tooltips on formula components. -> Justification: Deconstructs complex math into understandable parts without cluttering the UI, making it less intimidating.
        - Report Info: Algorithm steps. -> Goal: Organize. -> Viz: HTML/CSS flowchart. -> Interaction: None. -> Justification: Provides a high-level, easy-to-follow map of the process before the user delves into the detailed code.
        - Report Info: DPO Loss function dynamics. -> Goal: Engage/Teach. -> Viz: Interactive sliders and a Chart.js bar chart. -> Interaction: User manipulates sliders representing log probabilities and beta, causing the chart and calculated loss value to update in real-time. -> Justification: This is the core 'wow' factor. It transforms the abstract mathematical formula into a tangible, dynamic system, allowing users to build a strong, intuitive understanding of how different factors influence the DPO loss. Library: Chart.js on a Canvas element.
        - Report Info: General DPO concepts. -> Goal: Inform/Clarify. -> Viz: Text output. -> Interaction: User input text query, LLM generates explanation. Justification: Provides on-demand clarification for any DPO-related term or idea, leveraging LLM's knowledge. Library: Gemini API.
        - Report Info: DPO practical applications. -> Goal: Inspire/Contextualize. -> Viz: Text output (list). -> Interaction: Button click, LLM generates application scenarios. Justification: Connects theoretical knowledge to real-world utility, making the learning more relevant. Library: Gemini API.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body { font-family: 'Noto Sans SC', sans-serif; background-color: #F9F7F3; color: #3D3D3D; }
        .nav-link { transition: color 0.3s, border-color 0.3s; }
        .nav-link.active { color: #3B82F6; border-bottom-color: #3B82F6; }
        .nav-link:hover { color: #3B82F6; }
        .card { background-color: #FFFFFF; border-radius: 0.75rem; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1); padding: 1.5rem; transition: transform 0.3s, box-shadow 0.3s; }
        .card:hover { transform: translateY(-5px); box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1); }
        .flowchart-step { border: 2px solid #E5E7EB; padding: 1rem; border-radius: 0.5rem; text-align: center; }
        .flowchart-arrow { color: #9CA3AF; font-size: 2rem; margin: 0 1rem; }
        .code-block { background-color: #282C34; color: #ABB2BF; font-family: 'Courier New', Courier, monospace; padding: 1.5rem; border-radius: 0.5rem; overflow-x: auto; }
        .tooltip { position: relative; display: inline-block; border-bottom: 1px dotted #3D3D3D; cursor: help; }
        .tooltip .tooltiptext { visibility: hidden; width: 220px; background-color: #3D3D3D; color: #fff; text-align: center; border-radius: 6px; padding: 8px; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -110px; opacity: 0; transition: opacity 0.3s; font-size: 0.875rem; font-weight: 400; }
        .tooltip:hover .tooltiptext { visibility: visible; opacity: 1; }
        .chart-container { position: relative; height: 320px; width: 100%; max-width: 700px; margin: 2rem auto 0 auto; }
        input[type=range] { -webkit-appearance: none; width: 100%; background: transparent; }
        input[type=range]::-webkit-slider-thumb { -webkit-appearance: none; height: 16px; width: 16px; border-radius: 50%; background: #3B82F6; cursor: pointer; margin-top: -6px; }
        input[type=range]::-moz-range-thumb { height: 16px; width: 16px; border-radius: 50%; background: #3B82F6; cursor: pointer; }
        input[type=range]::-webkit-slider-runnable-track { width: 100%; height: 4px; cursor: pointer; background: #D1D5DB; border-radius: 2px; }
        input[type=range]::-moz-range-track { width: 100%; height: 4px; cursor: pointer; background: #D1D5DB; border-radius: 2px; }
        .llm-button { background-color: #3B82F6; color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; font-weight: 600; transition: background-color 0.3s ease; display: inline-flex; items-center; justify-center; }
        .llm-button:hover { background-color: #2563EB; }
        .llm-output { background-color: #F0F4F8; border-radius: 0.5rem; padding: 1rem; min-height: 80px; margin-top: 1rem; border: 1px solid #E2E8F0; }
        .loading-spinner { border: 4px solid rgba(0, 0, 0, 0.1); border-left-color: #3B82F6; border-radius: 50%; width: 24px; height: 24px; animation: spin 1s linear infinite; display: inline-block; vertical-align: middle; margin-right: 8px; }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="antialiased">

    <header class="bg-white/80 backdrop-blur-md shadow-sm sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <h1 class="text-xl font-bold text-gray-800">交互式DPO算法浏览器</h1>
            <div class="hidden md:flex space-x-8">
                <a href="#intro" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">介绍</a>
                <a href="#theory" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">核心理论</a>
                <a href="#flow" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">算法流程</a>
                <a href="#code" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">代码实现</a>
                <a href="#simulator" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">交互模拟</a>
                <a href="#llm-features" class="nav-link text-gray-600 font-medium border-b-2 border-transparent pb-1">AI助手</a>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-6 py-12">

        <section id="intro" class="mb-24 text-center">
            <h2 class="text-4xl font-bold mb-4">深入理解直接偏好优化 (DPO)</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto mb-12">
                DPO为大型语言模型（LLMs）对齐人类偏好提供了一种更直接、更稳定的新范式。本应用将通过可视化对比、理论分解和交互式模拟，带您探索DPO的工作原理，将复杂的概念变得直观易懂。
            </p>
            
            <div class="grid md:grid-cols-2 gap-8 items-center">
                <div class="card text-left">
                    <h3 class="text-2xl font-semibold mb-3 text-center">传统 RLHF 流程</h3>
                    <p class="text-gray-600 mb-4 text-center">这是一个多阶段、复杂的流程，依赖于一个独立训练的奖励模型。</p>
                    <div class="space-y-2">
                        <div class="flowchart-step bg-blue-50">1. 监督微调 (SFT)</div>
                        <div class="text-center text-gray-400">↓</div>
                        <div class="flowchart-step bg-yellow-50">2. 训练奖励模型 (RM)</div>
                        <div class="text-center text-gray-400">↓</div>
                        <div class="flowchart-step bg-green-50">3. PPO强化学习</div>
                    </div>
                </div>
                <div class="card text-left">
                    <h3 class="text-2xl font-semibold mb-3 text-center">DPO 流程</h3>
                    <p class="text-gray-600 mb-4 text-center">DPO将奖励学习和策略优化合并为一步，直接从偏好数据中学习。</p>
                    <div class="space-y-2">
                         <div class="flowchart-step bg-blue-50">1. 监督微调 (SFT)</div>
                        <div class="text-center text-gray-400">↓</div>
                        <div class="flowchart-step bg-purple-50">2. 直接偏好优化 (DPO)</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="theory" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-2">核心理论分解</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto text-center mb-12">DPO的优雅之处在于其坚实的数学基础。它将偏好学习问题巧妙地转化为一个简单的分类任务。我们来一步步拆解它的核心公式。</p>
            
            <div class="grid md:grid-cols-1 gap-8">
                <div class="card">
                    <h3 class="font-semibold text-xl mb-4">DPO 目标函数</h3>
                    <p class="text-gray-600 mb-6">DPO的目标是最大化所有偏好对的对数似然。这意味着，模型被激励去提高“获胜”响应($y_w$)的概率，同时降低“落败”响应($y_l$)的概率。</p>
                    <div class="code-block text-lg text-center p-8 bg-gray-100 text-gray-800">
                        <span class="font-serif italic">L</span><sub>DPO</sub> = 𝔼<sub>(x, y<sub>w</sub>, y<sub>l</sub>)∼D</sub> [ log σ( 
                        <span class="tooltip">β<span class="tooltiptext">温度系数β，控制对参考策略的偏离程度。β越高，策略越倾向于利用学到的偏好，但也可能偏离原始模型太远。</span></span>
                        ( log 
                        <span class="tooltip">π<sub>θ</sub>(y<sub>w</sub>|x)<span class="tooltiptext">策略模型πθ对“获胜”响应yw生成的对数概率。</span></span> 
                        - log 
                        <span class="tooltip">π<sub>ref</sub>(y<sub>w</sub>|x)<span class="tooltiptext">参考模型πref对“获胜”响应yw生成的对数概率。</span></span>
                        ) - 
                        ( log 
                        <span class="tooltip">π<sub>θ</sub>(y<sub>l</sub>|x)<span class="tooltiptext">策略模型πθ对“落败”响应yl生成的对数概率。</span></span> 
                        - log 
                        <span class="tooltip">π<sub>ref</sub>(y<sub>l</sub>|x)<span class="tooltiptext">参考模型πref对“落败”响应yl生成的对数概率。</span></span>
                        ) ) ]
                    </div>
                </div>
            </div>
        </section>

        <section id="flow" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-2">算法工作流程</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto text-center mb-12">DPO的训练循环非常直观。以下是每一步的核心操作，它将理论公式转化为可执行的计算过程。</p>
            <div class="flex flex-col md:flex-row items-center justify-center space-y-4 md:space-y-0 md:space-x-4">
                <div class="flowchart-step">1. 加载偏好数据<br>(x, y<sub>w</sub>, y<sub>l</sub>)</div>
                <div class="flowchart-arrow hidden md:block">→</div>
                <div class="flowchart-step">2. 计算对数概率<br>π<sub>θ</sub> 和 π<sub>ref</sub></div>
                <div class="flowchart-arrow hidden md:block">→</div>
                <div class="flowchart-step">3. 计算隐式奖励差</div>
                <div class="flowchart-arrow hidden md:block">→</div>
                <div class="flowchart-step">4. 计算DPO损失</div>
                <div class="flowchart-arrow hidden md:block">→</div>
                <div class="flowchart-step">5. 更新策略模型 π<sub>θ</sub></div>
            </div>
        </section>
        
        <section id="code" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-2">代码核心实现</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto text-center mb-12">理论的最终体现是代码。这里展示了DPO损失函数计算的核心Python代码，它精确地实现了上述公式和流程。</p>
            <div class="card">
                <h3 class="font-semibold text-xl mb-4">compute_dpo_loss 函数</h3>
                <div class="code-block">
<pre><code>
# 计算策略模型和参考模型对 chosen/rejected 响应的对数概率
# chosen_log_probs_policy, chosen_log_probs_ref
# rejected_log_probs_policy, rejected_log_probs_ref

# DPO损失的核心计算逻辑
logits = self.beta * (
    (chosen_log_probs_policy - chosen_log_probs_ref) - 
    (rejected_log_probs_policy - rejected_log_probs_ref)
)

# DPO损失函数是 -log(sigmoid(logits)) 的均值
loss = -F.logsigmoid(logits).mean()

return loss
</code></pre>
                </div>
            </div>
        </section>

        <section id="simulator" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-2">交互式损失模拟器</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto text-center mb-12">
                亲手操作来建立直觉！拖动下方的滑块，模拟策略模型 (π<sub>θ</sub>) 和参考模型 (π<sub>ref</sub>) 的对数概率变化。观察隐式奖励和最终DPO损失如何实时响应您的调整。
            </p>

            <div class="card">
                <div class="grid md:grid-cols-2 gap-8">
                    <div>
                        <div class="mb-4">
                            <label for="pi_theta_w" class="font-medium">策略模型对 y<sub>w</sub> 的对数概率: <span id="pi_theta_w_val">-1.0</span></label>
                            <input type="range" id="pi_theta_w" min="-5" max="0" value="-1" step="0.1" class="w-full">
                        </div>
                        <div class="mb-4">
                            <label for="pi_ref_w" class="font-medium">参考模型对 y<sub>w</sub> 的对数概率: <span id="pi_ref_w_val">-2.0</span></label>
                            <input type="range" id="pi_ref_w" min="-5" max="0" value="-2" step="0.1" class="w-full">
                        </div>
                        <div class="mb-4">
                            <label for="pi_theta_l" class="font-medium">策略模型对 y<sub>l</sub> 的对数概率: <span id="pi_theta_l_val">-3.0</span></label>
                            <input type="range" id="pi_theta_l" min="-5" max="0" value="-3" step="0.1" class="w-full">
                        </div>
                        <div class="mb-4">
                            <label for="pi_ref_l" class="font-medium">参考模型对 y<sub>l</sub> 的对数概率: <span id="pi_ref_l_val">-2.5</span></label>
                            <input type="range" id="pi_ref_l" min="-5" max="0" value="-2.5" step="0.1" class="w-full">
                        </div>
                        <div class="mb-4">
                            <label for="beta" class="font-medium">温度系数 β: <span id="beta_val">0.1</span></label>
                            <input type="range" id="beta" min="0.01" max="1" value="0.1" step="0.01" class="w-full">
                        </div>
                    </div>
                    <div class="flex flex-col items-center justify-center space-y-4">
                        <div class="text-center">
                            <p class="text-gray-500">隐式奖励 r(y<sub>w</sub>)</p>
                            <p class="text-2xl font-bold" id="reward_w">0.10</p>
                        </div>
                        <div class="text-center">
                            <p class="text-gray-500">隐式奖励 r(y<sub>l</sub>)</p>
                            <p class="text-2xl font-bold" id="reward_l">-0.05</p>
                        </div>
                        <div class="text-center p-4 bg-blue-50 rounded-lg">
                            <p class="text-gray-600 font-semibold">最终DPO损失 (越小越好)</p>
                            <p class="text-4xl font-extrabold text-blue-600" id="dpo_loss">0.686</p>
                        </div>
                    </div>
                </div>
                <div class="chart-container">
                    <canvas id="rewardChart"></canvas>
                </div>
            </div>
        </section>

        <section id="llm-features" class="mb-24">
            <h2 class="text-3xl font-bold text-center mb-2">✨ AI 助手 ✨</h2>
            <p class="text-lg text-gray-600 max-w-3xl mx-auto text-center mb-12">
                利用AI的力量，更深入地探索DPO算法！无论是概念疑问还是应用场景，AI助手都能为您提供即时洞察。
            </p>

            <div class="grid md:grid-cols-2 gap-8">
                <div class="card">
                    <h3 class="font-semibold text-xl mb-4">✨ DPO概念解释器 ✨</h3>
                    <p class="text-gray-600 mb-4">输入您对DPO的任何疑问，AI将为您提供简洁的解释。</p>
                    <textarea id="concept-query" class="w-full p-3 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500" rows="3" placeholder="例如：DPO中的β参数有什么作用？"></textarea>
                    <button id="explain-concept-btn" class="llm-button mt-4">
                        <span id="explain-concept-spinner" class="loading-spinner hidden"></span>
                        ✨ 解释概念 ✨
                    </button>
                    <div id="concept-output" class="llm-output mt-4 text-gray-700"></div>
                </div>

                <div class="card">
                    <h3 class="font-semibold text-xl mb-4">✨ DPO应用场景建议 ✨</h3>
                    <p class="text-gray-600 mb-4">点击按钮，获取DPO在大型语言模型对齐中的实际应用场景建议。</p>
                    <button id="suggest-scenarios-btn" class="llm-button mt-4">
                        <span id="suggest-scenarios-spinner" class="loading-spinner hidden"></span>
                        ✨ 获取场景建议 ✨
                    </button>
                    <div id="scenarios-output" class="llm-output mt-4 text-gray-700"></div>
                </div>
            </div>
        </section>

    </main>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const sliders = {
                pi_theta_w: document.getElementById('pi_theta_w'),
                pi_ref_w: document.getElementById('pi_ref_w'),
                pi_theta_l: document.getElementById('pi_theta_l'),
                pi_ref_l: document.getElementById('pi_ref_l'),
                beta: document.getElementById('beta'),
            };

            const values = {
                pi_theta_w_val: document.getElementById('pi_theta_w_val'),
                pi_ref_w_val: document.getElementById('pi_ref_w_val'),
                pi_theta_l_val: document.getElementById('pi_theta_l_val'),
                pi_ref_l_val: document.getElementById('pi_ref_l_val'),
                beta_val: document.getElementById('beta_val'),
            };

            const outputs = {
                reward_w: document.getElementById('reward_w'),
                reward_l: document.getElementById('reward_l'),
                dpo_loss: document.getElementById('dpo_loss'),
            };

            const ctx = document.getElementById('rewardChart').getContext('2d');
            const rewardChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['获胜响应 (yw)', '落败响应 (yl)'],
                    datasets: [{
                        label: '隐式奖励',
                        data: [0, 0],
                        backgroundColor: [
                            'rgba(59, 130, 246, 0.6)',
                            'rgba(239, 68, 68, 0.6)',
                        ],
                        borderColor: [
                            'rgba(59, 130, 246, 1)',
                            'rgba(239, 68, 68, 1)',
                        ],
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: false,
                            title: {
                                display: true,
                                text: '奖励值'
                            }
                        }
                    },
                    plugins: {
                        legend: {
                            display: false
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `奖励: ${context.parsed.y.toFixed(3)}`;
                                }
                            }
                        }
                    }
                }
            });

            function calculateDPO() {
                const pi_theta_w = parseFloat(sliders.pi_theta_w.value);
                const pi_ref_w = parseFloat(sliders.pi_ref_w.value);
                const pi_theta_l = parseFloat(sliders.pi_theta_l.value);
                const pi_ref_l = parseFloat(sliders.pi_ref_l.value);
                const beta = parseFloat(sliders.beta.value);

                values.pi_theta_w_val.textContent = pi_theta_w.toFixed(2);
                values.pi_ref_w_val.textContent = pi_ref_w.toFixed(2);
                values.pi_theta_l_val.textContent = pi_theta_l.toFixed(2);
                values.pi_ref_l_val.textContent = pi_ref_l.toFixed(2);
                values.beta_val.textContent = beta.toFixed(2);

                const reward_w = beta * (pi_theta_w - pi_ref_w);
                const reward_l = beta * (pi_theta_l - pi_ref_l);
                
                const logits = reward_w - reward_l;
                const loss = -Math.log(1 / (1 + Math.exp(-logits)));

                outputs.reward_w.textContent = reward_w.toFixed(3);
                outputs.reward_l.textContent = reward_l.toFixed(3);
                outputs.dpo_loss.textContent = loss.toFixed(3);
                
                rewardChart.data.datasets[0].data = [reward_w, reward_l];
                rewardChart.update();
            }
            
            Object.values(sliders).forEach(slider => {
                slider.addEventListener('input', calculateDPO);
            });

            calculateDPO();

            const navLinks = document.querySelectorAll('.nav-link');
            const sections = document.querySelectorAll('section');
            
            window.addEventListener('scroll', () => {
                let current = '';
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    if (pageYOffset >= sectionTop - 100) {
                        current = section.getAttribute('id');
                    }
                });

                navLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href').includes(current)) {
                        link.classList.add('active');
                    }
                });
            });

            navLinks.forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                });
            });

            // LLM Integration
            const conceptQueryInput = document.getElementById('concept-query');
            const explainConceptBtn = document.getElementById('explain-concept-btn');
            const conceptOutput = document.getElementById('concept-output');
            const explainConceptSpinner = document.getElementById('explain-concept-spinner');

            const suggestScenariosBtn = document.getElementById('suggest-scenarios-btn');
            const scenariosOutput = document.getElementById('scenarios-output');
            const suggestScenariosSpinner = document.getElementById('suggest-scenarios-spinner');

            async function callGeminiAPI(prompt, outputElement, spinnerElement) {
                outputElement.innerHTML = ''; // Clear previous output
                spinnerElement.classList.remove('hidden'); // Show spinner

                let chatHistory = [];
                chatHistory.push({ role: "user", parts: [{ text: prompt }] });
                const payload = { contents: chatHistory };
                const apiKey = ""; // Canvas will provide this at runtime
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

                try {
                    const response = await fetch(apiUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload)
                    });
                    const result = await response.json();

                    if (result.candidates && result.candidates.length > 0 &&
                        result.candidates[0].content && result.candidates[0].content.parts &&
                        result.candidates[0].content.parts.length > 0) {
                        const text = result.candidates[0].content.parts[0].text;
                        outputElement.innerHTML = text.replace(/\n/g, '<br>'); // Display newlines as <br>
                    } else {
                        outputElement.textContent = '未能获取到有效回复，请重试。';
                        console.error('Gemini API response structure unexpected:', result);
                    }
                } catch (error) {
                    outputElement.textContent = '调用API时发生错误，请检查网络或稍后再试。';
                    console.error('Error calling Gemini API:', error);
                } finally {
                    spinnerElement.classList.add('hidden'); // Hide spinner
                }
            }

            explainConceptBtn.addEventListener('click', () => {
                const query = conceptQueryInput.value.trim();
                if (query) {
                    const prompt = `请用中文简洁地解释DPO算法中的"${query}"。`;
                    callGeminiAPI(prompt, conceptOutput, explainConceptSpinner);
                } else {
                    conceptOutput.textContent = '请输入您想了解的概念。';
                }
            });

            suggestScenariosBtn.addEventListener('click', () => {
                const prompt = "请列举3-5个DPO算法在大型语言模型（LLM）对齐中的具体应用场景，用中文以列表形式呈现。";
                callGeminiAPI(prompt, scenariosOutput, suggestScenariosSpinner);
            });
        });
    </script>

</body>
</html>
